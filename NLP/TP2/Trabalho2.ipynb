{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e2a9NGsJQAe"
      },
      "source": [
        "# Trabalho 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwwH1iriJS56"
      },
      "source": [
        "## Sobre o trabalho\n",
        "- O trabalho tem os seguintes objetivos:\n",
        "    - Estudar a tarefa de **POS Tagging** (Part-Of-Speech (POS) Tagging) para a língua Portuguesa\n",
        "    - Classificar a classe gramatical de palavras em Português\n",
        "    - Implementar e avaliar a precisão de um modelo de POS tagging\n",
        "\n",
        "## Resumo do trabalho\n",
        "- O modelo utilizado foi o BERT\n",
        "- Foi feito um finetuning no modelo, para classificação de classe gramatical para cada palavra\n",
        "\n",
        "### Ferramentas, dados e bibliotecas utilizadas\n",
        "- O corpus de treino, avaliação e teste utilizado foi o corpus recomendado na descrição do trabalho: **macmorpho**\n",
        "- A principal biblioteca utilizada para manipular o modelo e os dados foi a biblioteca recomendada: `transformers`\n",
        "  - Outras bibliotecas úteis utilizadas: `torch`, `numpy` e `scikit-learn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXkkoeeUJaQF",
        "outputId": "cf14674f-989b-4e0e-abfa-ebe55c0f5b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/265.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# %pip install transformers[torch]\n",
        "%pip install accelerate -U --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "40TJENoRnrLN"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "# Set logging level\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logging.getLogger().addHandler(logging.StreamHandler())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHwvhgQgJfT-"
      },
      "source": [
        "## Carregar e tokenizar os dados de treino\n",
        "\n",
        "#### Pipeline de separação dos dados\n",
        "- É usado o método `preprocess_and_tokenize()` para preprocessar e tokenizar os dados\n",
        "1. Dados são lidos linha a linha\n",
        "2. Depois, a linha é separada em espaços\n",
        "3. Depois, é separada novamente por `_`\n",
        "4. Dessa forma, obtém se a palavra e a classe gramatical dela\n",
        "5. O processo é repetido para todo o conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7Nm4dEs8IivV"
      },
      "outputs": [],
      "source": [
        "# Create a device object\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Set max_length\n",
        "max_length = 512\n",
        "\n",
        "def preprocess_and_tokenize(data):\n",
        "    \"\"\"Preprocesses and tokenizes the input data.\n",
        "    Returns tokenized inputs, padded POS tags, and attention masks. The\n",
        "    input_ids and padded_pos_tags are padded to `max_length`. There is a\n",
        "    one-to-one correspondence between the tokens, POS tags and\n",
        "    attention masks.\\\\\n",
        "    Arguments:\n",
        "        data: list of strings. Each string is a sentence with words and POS\n",
        "            tags separated by an underscore. For example,\n",
        "            \"Jersei_N atinge_V média_N de_PREP\".\n",
        "    Returns:\n",
        "        input_ids: torch.tensor of shape (num_sentences, max_length).\n",
        "            It is a \"list\" of numerical values (ids) that represent each token\n",
        "            in the input text. The values are based on the vocabulary of the\n",
        "            pre-trained BERT model.\n",
        "        padded_pos_tags: torch.tensor of shape (num_sentences, max_length).\n",
        "            It is a \"list\" of numerical values (ids) that represent each POS\n",
        "            tag in the input text. The values are based on the label encoder.\n",
        "        attention_masks: torch.tensor of shape (num_sentences, max_length).\n",
        "            It is a \"list\" of 0s and 1s. The 1s indicate the position of the\n",
        "            tokens in the input_ids tensor. The 0s indicate the padding.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    pos_tags = []\n",
        "    unique_tags = set()\n",
        "\n",
        "    # Split data into sentences and POS tags\n",
        "    # count = 0\n",
        "    for line in tqdm(data, desc=\"Splitting data into sentences and POS tags\"):\n",
        "        words = []\n",
        "        tags = []\n",
        "        logging.debug(f\"line: {line}\")\n",
        "        for word in line.split():\n",
        "            split_word = word.split('_')\n",
        "            logging.debug(f\"split_word: {split_word}\")\n",
        "            words.append(split_word[0])\n",
        "            tags.append(split_word[1])\n",
        "            unique_tags.add(split_word[1])\n",
        "            logging.debug(f\"words: {words}\")\n",
        "            logging.debug(f\"unique_tags: {unique_tags}\")\n",
        "        sentences.append(words)\n",
        "        pos_tags.append(tags)\n",
        "        # count += 1\n",
        "        # if count > 50:\n",
        "        #     break\n",
        "\n",
        "    # Count the number of sentences and tags\n",
        "    num_sentences = sum(len(sentence) for sentence in sentences)\n",
        "    num_tags = sum(len(tags) for tags in pos_tags)\n",
        "    print(f\"Number of sentences: {num_sentences}\")\n",
        "    logging.debug(f\"Number of tags (classes): {num_tags}\")\n",
        "    print(f\"Number of unique tags: {len(unique_tags)}\")\n",
        "\n",
        "    # Tokenize the sentences\n",
        "    tokenized_inputs = bert_tokenizer(sentences, truncation=True, padding='max_length',\n",
        "                                      max_length=max_length, is_split_into_words=True)\n",
        "\n",
        "    # Pad input_ids to max_length\n",
        "    input_ids = pad_sequence([torch.tensor(i) for i in tokenized_inputs[\"input_ids\"]], batch_first=True)\n",
        "    input_ids = input_ids.to(device)  # Move to GPU\n",
        "    # FIXME: Do this next to avoid unnecessary conversions\n",
        "    # input_ids = pad_sequence(tokenized_inputs[\"input_ids\"], batch_first=True).to(device)\n",
        "    logging.debug(f\"input_ids: {input_ids}\")\n",
        "\n",
        "    # Handle the POS tags\n",
        "    new_pos_tags = []\n",
        "    for sent_tags, input_id in zip(pos_tags, tokenized_inputs[\"input_ids\"]):\n",
        "        new_tags = []\n",
        "        for tag in sent_tags:\n",
        "            new_tags.extend([tag] * len(input_id))\n",
        "        new_pos_tags.append(new_tags[:len(input_id)])\n",
        "\n",
        "    # Fit the label encoder with the unique tags\n",
        "    label_encoder.fit(list(unique_tags))\n",
        "    # Encode the POS tags\n",
        "    # encoded_pos_tags = [label_encoder.fit_transform(tags) for tags in new_pos_tags]\n",
        "    encoded_pos_tags = [label_encoder.transform(tags) for tags in pos_tags]\n",
        "    logging.debug(f\"encoded_pos_tags: {encoded_pos_tags}\")\n",
        "\n",
        "    # Pad the POS tags\n",
        "    # padded_pos_tags = pad_sequence([torch.tensor(tags) for tags in encoded_pos_tags], batch_first=True)\n",
        "    # padded_pos_tags = padded_pos_tags.to(device)  # Move to GPU\n",
        "    padded_pos_tags = [F.pad(torch.tensor(tags), pad=(0, max_length - len(tags))) for tags in encoded_pos_tags]\n",
        "    padded_pos_tags = torch.stack(padded_pos_tags).to(device)  # Stack the list of tensors into a single tensor and move to GPU\n",
        "    logging.debug(f\"padded_pos_tags: {padded_pos_tags}\")\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = [[float(i != 0.0) for i in seq] for seq in input_ids]\n",
        "    attention_masks = torch.tensor(attention_masks).to(device)  # Move to GPU\n",
        "    logging.debug(f\"attention_masks: {attention_masks}\")\n",
        "\n",
        "    print(f\"Number of sequences (dataset lines): {input_ids.shape[0]}\")\n",
        "    print()\n",
        "\n",
        "    return input_ids, padded_pos_tags, attention_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3cGhKC6Jjo9"
      },
      "source": [
        "### Montar Google Drive e alterar para o diretório principal do código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLuAkrkyJkKn",
        "outputId": "d88cdb47-90dd-448e-c893-3d12c19f4c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# change execution folder to MyDrive/UFMG/NLP/\n",
        "import os\n",
        "os.chdir('drive/MyDrive/UFMG/NLP/')\n",
        "# os.getcwd()\n",
        "# os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOEFJwqnzHS"
      },
      "source": [
        "### Carrega id, tags e masks do conjunto de treino e de validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74JpbpbslqYl"
      },
      "source": [
        "- O número de sentenças usadas no conjunto de treino foram de 728497 sentenças\n",
        "- Em um total de 37948 sequências, isto é, frases completas\n",
        "- O número de tags (classes gramaticais) únicas é 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KUCLLN3JnaG",
        "outputId": "1dac8095-eaf8-47ce-ccac-4da9bf1c8e17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Splitting data into sentences and POS tags: 100%|██████████| 37948/37948 [00:13<00:00, 2786.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 728497\n",
            "Number of unique tags: 26\n",
            "Number of sequences (dataset lines): 37948\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Splitting data into sentences and POS tags: 100%|██████████| 1997/1997 [00:00<00:00, 3494.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 38881\n",
            "Number of unique tags: 26\n",
            "Number of sequences (dataset lines): 1997\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('data/macmorpho-train.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    input_ids, tags, masks = preprocess_and_tokenize(data)\n",
        "\n",
        "with open('data/macmorpho-dev.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    val_input_ids, val_tags, val_masks = preprocess_and_tokenize(data)\n",
        "\n",
        "UNIQUE_TAGS = 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZL9o0NGJqlt",
        "outputId": "c57a3595-2335-4200-b2c4-ab88ff731a6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([37948, 512]), torch.Size([37948, 512]), 37948)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, tags.shape, len(masks)\n",
        "# len(val_input_ids), val_tags.shape, len(val_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcsWp3xcJla-"
      },
      "source": [
        "## Finetuning do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yJiqBFMJs9n"
      },
      "source": [
        "#### Modelo BERT `bert-base-multilingual-cased`\n",
        "- 179M parâmetros treináveis\n",
        "- Modelo pré-treinado em 104 idiomas, incluindo o português\n",
        "  - Versão \"cased\" do modelo, o que ajuda em POS Tagging\n",
        "- Modo de treinamento: Finetuning apenas na última camada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d83222b6d94344cf93d4d5081018ab92",
            "df765c99aa014d75bebcb87e3621d918",
            "1adf3f119e404bff858808e83df4ffeb",
            "818a1eafc177434dba3ef3d140e6eaf8",
            "bb17a27efa4e42b1b03cc3b2d0b40910",
            "74c2146ce26b41df854a86a17cc53b8a",
            "65da04dba8c4476ca781c9e8914493b6",
            "a863952591f1476a83d4be55cb10e676",
            "e13c9ae71e7c4142a63c75a72439ab07",
            "e32c9968614849cd8e4bc3759e3d6f53",
            "34e94c4bc74b490d92a877829be2f952"
          ]
        },
        "id": "xNtGe4oVJxnO",
        "outputId": "bfc1ed50-9655-4179-c0ff-190033a40591"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d83222b6d94344cf93d4d5081018ab92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Define a custom PyTorch Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_ids, masks, tags):\n",
        "        self.input_ids = input_ids\n",
        "        self.masks = masks\n",
        "        self.tags = tags.to('cpu')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.masks[idx],\n",
        "            'labels': self.tags[idx]\n",
        "        }\n",
        "\n",
        "# Move the tensors to the CPU\n",
        "input_ids_cpu = input_ids.to('cpu')\n",
        "masks_cpu = masks.to('cpu')\n",
        "tags_cpu = tags.to('cpu')\n",
        "val_input_ids_cpu = val_input_ids.to('cpu')\n",
        "val_masks_cpu = val_masks.to('cpu')\n",
        "val_tags_cpu = val_tags.to('cpu')\n",
        "\n",
        "# Convert training data into PyTorch Dataset\n",
        "# train_dataset = CustomDataset(input_ids, masks, tags)\n",
        "train_dataset = CustomDataset(input_ids_cpu, masks_cpu, tags_cpu)\n",
        "\n",
        "# Convert validation data into PyTorch Dataset\n",
        "# eval_dataset = CustomDataset(val_input_ids, val_masks, val_tags)\n",
        "eval_dataset = CustomDataset(val_input_ids_cpu, val_masks_cpu, val_tags_cpu)\n",
        "\n",
        "# Initialize the BERT model\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    num_labels=UNIQUE_TAGS,\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16, # 4, 16\n",
        "    per_device_eval_batch_size=32,  # 4, 32, 64\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1000,  # Set logging_steps to see the validation loss and metrics\n",
        "    # bf16=True, # or fp16, to reduce memory/computer usage/requirements\n",
        "    save_strategy=\"epoch\",\n",
        "    dataloader_num_workers=3,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Define the evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    labels = np.ravel(eval_pred.label_ids)\n",
        "    preds = np.ravel(eval_pred.predictions.argmax(-1))\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogc-cFu5Jzk2",
        "outputId": "3577cbb6-8961-4d7b-c29c-20e0ddd3adcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification_layer: Linear(in_features=768, out_features=26, bias=True)\n"
          ]
        }
      ],
      "source": [
        "classification_layer = model.classifier\n",
        "print(f\"classification_layer: {classification_layer}\")\n",
        "\n",
        "# Freeze all the layers of the BERT model\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Train only the classifier layer\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name, param.requires_grad)\n",
        "\n",
        "# Check that only the classifier layer is trainable\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' in name:  # Check if the parameter belongs to the classifier layer\n",
        "        assert param.requires_grad == True  # Assert that the classifier layer parameters are trainable\n",
        "    else:\n",
        "        assert param.requires_grad == False  # Assert that all other parameters are not trainable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDkI17nyt_10"
      },
      "source": [
        "# Agora sim, treinando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "n6VrOEXRJ1sJ",
        "outputId": "8bce360d-456a-4f29-f444-f5e7eb5aefd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4278' max='7116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4278/7116 42:53 < 28:28, 1.66 it/s, Epoch 1.80/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.196700</td>\n",
              "      <td>0.160081</td>\n",
              "      <td>0.964116</td>\n",
              "      <td>0.938580</td>\n",
              "      <td>0.964116</td>\n",
              "      <td>0.949728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train_output = trainer.train()\n",
        "\n",
        "# Print the training loss\n",
        "print(train_output.training_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd5Xx204vm55"
      },
      "source": [
        "#### Deletar dados de treino/validação na gpu para liberar espaço"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "w8k-lMTGvH-T"
      },
      "outputs": [],
      "source": [
        "# Delete the training/validation data\n",
        "del train_dataset\n",
        "del eval_dataset\n",
        "del input_ids, tags, masks,\n",
        "del val_input_ids, val_tags, val_masks\n",
        "\n",
        "# Empty the cache\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NeOOVE1r4M"
      },
      "source": [
        "### Observar logs de treino usando tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f476L2nlsnoe"
      },
      "outputs": [],
      "source": [
        "!tensorboard --logdir=./logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU9-omxVJ38v"
      },
      "source": [
        "## Carrega o conjunto de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyTpZHLZJ4cO",
        "outputId": "c7819845-99b9-480f-e315-9124d3ee31ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Splitting data into sentences and POS tags: 100%|██████████| 9987/9987 [00:03<00:00, 3227.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 178373\n",
            "Number of unique tags: 26\n",
            "Number of sequences (dataset lines): 9987\n"
          ]
        }
      ],
      "source": [
        "with open('data/macmorpho-test.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    test_input_ids, test_tags, test_masks = preprocess_and_tokenize(data)\n",
        "\n",
        "test_input_ids_cpu = test_input_ids.to('cpu')\n",
        "test_masks_cpu = test_masks.to('cpu')\n",
        "test_tags_cpu = test_tags.to('cpu')\n",
        "\n",
        "# Convert test data into PyTorch Dataset\n",
        "test_dataset = CustomDataset(test_input_ids_cpu, test_masks_cpu, test_tags_cpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC5iMQFFp7tV"
      },
      "source": [
        "### Avalia o modelo treinado nos dados de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbO69aDgqBv2"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "trainer.args.per_device_eval_batch_size = 4  # reduce evaluation batch size\n",
        "results = trainer.evaluate(test_dataset)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPu5klSvuRq-"
      },
      "source": [
        "## Agora vamos avaliar o modelo com relação a outras métricas\n",
        "- Vamos ver quais foram as palavras com melhores resultados\n",
        "- Vamos ver quais foram as palavras com piores resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71d9sKBEuhse"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "predictions, labels, _ = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_XDP3s3uajg"
      },
      "outputs": [],
      "source": [
        "def get_words_from_input_ids(input_ids):\n",
        "    words = []\n",
        "    for ids in input_ids:\n",
        "        words.append(bert_tokenizer.convert_ids_to_tokens(ids))\n",
        "    return words\n",
        "\n",
        "# Obtém as palavras a partir dos tokens\n",
        "words = get_words_from_input_ids(test_input_ids)\n",
        "\n",
        "# Apply softmax to get probabilities and then get the most probable tags\n",
        "predictions = np.argmax(F.softmax(torch.from_numpy(predictions), dim=-1), axis=-1)\n",
        "\n",
        "# Flatten predictions and labels\n",
        "predictions = predictions.flatten()\n",
        "labels = labels.flatten()\n",
        "\n",
        "# Create a DataFrame with columns 'true', 'predicted', 'word'\n",
        "df = pd.DataFrame({\n",
        "    'true': [tags[val] for val in labels],\n",
        "    'predicted': [tags[val] for val in predictions],\n",
        "    'word': [words[val] for val in input_ids_cpu.flatten()]\n",
        "})\n",
        "\n",
        "# Find the words where the true label is equal to the predicted label\n",
        "correct_predictions = df[df['true'] == df['predicted']]\n",
        "\n",
        "# Find the words where the true label is not equal to the predicted label\n",
        "incorrect_predictions = df[df['true'] != df['predicted']]\n",
        "\n",
        "print(\"## Palavras que tiveram a classe gramatical corretamente acertadas\")\n",
        "print(correct_predictions['word'].value_counts().head())\n",
        "\n",
        "print()\n",
        "print(\"## Palavras onde a predição da classe gramatical foi errada, por pelo menos uma vez\")\n",
        "print(incorrect_predictions['word'].value_counts().head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpkqiDpzJ6eV"
      },
      "source": [
        "#### Salvar modelo treinado na downstream task (POS Tagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "5WvWw2F8J8CM"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned BERT model\n",
        "trainer.save_model(\"my_finetuned_bert_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvBtXlG_J9vV"
      },
      "source": [
        "# Referências\n",
        "- [Hugging Face: Transformers - BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
        "- [Hugging Face: BERT multilingual base model (cased)](https://huggingface.co/bert-base-multilingual-cased)\n",
        "- Slides da disciplina"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1adf3f119e404bff858808e83df4ffeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a863952591f1476a83d4be55cb10e676",
            "max": 714290682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e13c9ae71e7c4142a63c75a72439ab07",
            "value": 714290682
          }
        },
        "34e94c4bc74b490d92a877829be2f952": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65da04dba8c4476ca781c9e8914493b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74c2146ce26b41df854a86a17cc53b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "818a1eafc177434dba3ef3d140e6eaf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32c9968614849cd8e4bc3759e3d6f53",
            "placeholder": "​",
            "style": "IPY_MODEL_34e94c4bc74b490d92a877829be2f952",
            "value": " 714M/714M [00:20&lt;00:00, 32.1MB/s]"
          }
        },
        "a863952591f1476a83d4be55cb10e676": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb17a27efa4e42b1b03cc3b2d0b40910": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d83222b6d94344cf93d4d5081018ab92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df765c99aa014d75bebcb87e3621d918",
              "IPY_MODEL_1adf3f119e404bff858808e83df4ffeb",
              "IPY_MODEL_818a1eafc177434dba3ef3d140e6eaf8"
            ],
            "layout": "IPY_MODEL_bb17a27efa4e42b1b03cc3b2d0b40910"
          }
        },
        "df765c99aa014d75bebcb87e3621d918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c2146ce26b41df854a86a17cc53b8a",
            "placeholder": "​",
            "style": "IPY_MODEL_65da04dba8c4476ca781c9e8914493b6",
            "value": "model.safetensors: 100%"
          }
        },
        "e13c9ae71e7c4142a63c75a72439ab07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e32c9968614849cd8e4bc3759e3d6f53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
