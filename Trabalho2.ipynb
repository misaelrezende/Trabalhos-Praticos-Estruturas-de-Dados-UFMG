{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e2a9NGsJQAe"
      },
      "source": [
        "# Trabalho 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwwH1iriJS56"
      },
      "source": [
        "### Sobre o trabalho\n",
        "- O trabalho tem os seguintes objetivos:\n",
        "    - Estudar a tarefa de **POS Tagging** (Part-Of-Speech (POS) Tagging) para a língua Portuguesa\n",
        "    - Classificar a classe gramatical de palavras em Português\n",
        "    - Implementar e avaliar a precisão de um modelo de POS tagging\n",
        "\n",
        "### Resumo do trabalho\n",
        "- O modelo utilizado foi o BERT\n",
        "- Foi feito um finetuning no modelo, para classificação de classe gramatical\n",
        "\n",
        "### Ferramentas, dados e bibliotecas utilizadas\n",
        "- O corpus de treino, avaliação e teste utilizado foi o corpus recomendado na descrição do trabalho: macmorpho\n",
        "- A principal biblioteca utilizada para manipular os dados foi a biblioteca recomendada: `transformers`\n",
        "  - Outras bibliotecas úteis utilizadas: `torch`, `numpy` e `scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9J2KkN4JV0Y"
      },
      "source": [
        "Trabalho Prático II\n",
        "\n",
        "Nosso objetivo é estudar a tarefa de POS Tagging para a língua Portuguesa. Para isso utilizaremos o corpus Mac-Morpho, produzido pelo grupo NILC da ICMC USP. O Mac-Morpho oferece dados para treinamento, validação e teste de modelos preditivos, capazes de classificar a classe gramatical de palavras em Português. Para acessar o conjunto de classes, acesse http://nilc.icmc.usp.br/macmorpho/macmorpho-manual.pdf.\n",
        "\n",
        "\n",
        "O corpus está disponível em http://nilc.icmc.usp.br/macmorpho/macmorpho-v3.tgz. Você deverá implementar e avaliar a precisão de um modelo de POS tagging, a sua escolha. Voce pode utilizar pacotes que facilitem a implementação, como gensim, e transformers. Você deverá entregar documentação embutida em um notebook, detalhando a tarefa, a metodologia e os resultados. Sua análise deverá discutir quais as classes gramaticais para as quais a precisão é maior/menor. Não utilizaremos LLMs para essa tarefa, mas a sugestão é utilizar Transformers disponiveis e pre-treinados, em especial o BERT.\n",
        "\n",
        "Exemplo de dados de treino:\n",
        "```\n",
        "Jersei_N atinge_V média_N de_PREP Cr$_CUR 1,4_NUM milhão_N na_PREP+ART venda_N da_PREP+ART Pinhal_NPROP em_PREP São_NPROP Paulo_NPROP ._PU Programe_V sua_PROADJ viagem_N à_PREP+ART Exposição_NPROP Nacional_NPROP do_NPROP Zebu_NPROP ,_PU que_PRO-KS começa_V dia_N 25_N ._PU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbcLvNSQJahc"
      },
      "source": [
        "# Discussão\n",
        "\n",
        "#### Geral\n",
        "- Quantos parametros treináveis tem o modelo no total?\n",
        "- Dá pra alterar mais alguma coisa do treino que possa vir a melhorar algo?\n",
        "- Há algo mais a melhorar nos parâmetros de treinamento?\n",
        "\n",
        "#### Específico para avaliação do trabalho\n",
        "- `Implementar e avaliar a precisão:` isso pra mim é falar de acurácia.\n",
        "- `detalhando a tarefa, metodologia e os resultados`\n",
        "- `discutir quais classes gramaticais tem maior/menor precisão`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXkkoeeUJaQF",
        "outputId": "18141af8-cf67-4b17-9d3f-bb1be06d1553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/265.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# %pip install transformers[torch]\n",
        "%pip install accelerate -U --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHwvhgQgJfT-"
      },
      "source": [
        "## Carregar e tokenizar os dados de treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7Nm4dEs8IivV"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "# Set logging level\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logging.getLogger().addHandler(logging.StreamHandler())\n",
        "\n",
        "# Create a device object\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Set max_length\n",
        "max_length = 512\n",
        "\n",
        "def preprocess_and_tokenize(data):\n",
        "    \"\"\"Preprocesses and tokenizes the input data.\n",
        "    Returns tokenized inputs, padded POS tags, and attention masks. The\n",
        "    input_ids and padded_pos_tags are padded to `max_length`. There is a\n",
        "    one-to-one correspondence between the tokens, POS tags and\n",
        "    attention masks.\\\\\n",
        "    Arguments:\n",
        "        data: list of strings. Each string is a sentence with words and POS\n",
        "            tags separated by an underscore. For example,\n",
        "            \"Jersei_N atinge_V média_N de_PREP\".\n",
        "    Returns:\n",
        "        input_ids: torch.tensor of shape (num_sentences, max_length).\n",
        "            It is a \"list\" of numerical values (ids) that represent each token\n",
        "            in the input text. The values are based on the vocabulary of the\n",
        "            pre-trained BERT model.\n",
        "        padded_pos_tags: torch.tensor of shape (num_sentences, max_length).\n",
        "            It is a \"list\" of numerical values (ids) that represent each POS\n",
        "            tag in the input text. The values are based on the label encoder.\n",
        "        attention_masks: torch.tensor of shape (num_sentences, max_length).\n",
        "            It is a \"list\" of 0s and 1s. The 1s indicate the position of the\n",
        "            tokens in the input_ids tensor. The 0s indicate the padding.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    pos_tags = []\n",
        "    unique_tags = set()\n",
        "\n",
        "    # Split data into sentences and POS tags\n",
        "    # count = 0\n",
        "    for line in tqdm(data, desc=\"Splitting data into sentences and POS tags\"):\n",
        "        words = []\n",
        "        tags = []\n",
        "        logging.debug(f\"line: {line}\")\n",
        "        for word in line.split():\n",
        "            split_word = word.split('_')\n",
        "            logging.debug(f\"split_word: {split_word}\")\n",
        "            words.append(split_word[0])\n",
        "            tags.append(split_word[1])\n",
        "            unique_tags.add(split_word[1])\n",
        "            logging.debug(f\"words: {words}\")\n",
        "            logging.debug(f\"unique_tags: {unique_tags}\")\n",
        "        sentences.append(words)\n",
        "        pos_tags.append(tags)\n",
        "        # count += 1\n",
        "        # if count > 50:\n",
        "        #     break\n",
        "\n",
        "    # Count the number of sentences and tags\n",
        "    num_sentences = sum(len(sentence) for sentence in sentences)\n",
        "    num_tags = sum(len(tags) for tags in pos_tags)\n",
        "    print(f\"Number of sentences: {num_sentences}\")\n",
        "    logging.debug(f\"Number of tags (classes): {num_tags}\")\n",
        "    print(f\"Number of unique tags: {len(unique_tags)}\")\n",
        "\n",
        "    # Tokenize the sentences\n",
        "    tokenized_inputs = bert_tokenizer(sentences, truncation=True, padding='max_length',\n",
        "                                      max_length=max_length, is_split_into_words=True)\n",
        "\n",
        "    # Pad input_ids to max_length\n",
        "    input_ids = pad_sequence([torch.tensor(i) for i in tokenized_inputs[\"input_ids\"]], batch_first=True)\n",
        "    input_ids = input_ids.to(device)  # Move to GPU\n",
        "    # FIXME: Do this next to avoid unnecessary conversions\n",
        "    # input_ids = pad_sequence(tokenized_inputs[\"input_ids\"], batch_first=True).to(device)\n",
        "    logging.debug(f\"input_ids: {input_ids}\")\n",
        "\n",
        "    # Handle the POS tags\n",
        "    new_pos_tags = []\n",
        "    for sent_tags, input_id in zip(pos_tags, tokenized_inputs[\"input_ids\"]):\n",
        "        new_tags = []\n",
        "        for tag in sent_tags:\n",
        "            new_tags.extend([tag] * len(input_id))\n",
        "        new_pos_tags.append(new_tags[:len(input_id)])\n",
        "\n",
        "    # Fit the label encoder with the unique tags\n",
        "    label_encoder.fit(list(unique_tags))\n",
        "    # Encode the POS tags\n",
        "    # encoded_pos_tags = [label_encoder.fit_transform(tags) for tags in new_pos_tags]\n",
        "    encoded_pos_tags = [label_encoder.transform(tags) for tags in pos_tags]\n",
        "    logging.debug(f\"encoded_pos_tags: {encoded_pos_tags}\")\n",
        "\n",
        "    # Pad the POS tags\n",
        "    # padded_pos_tags = pad_sequence([torch.tensor(tags) for tags in encoded_pos_tags], batch_first=True)\n",
        "    # padded_pos_tags = padded_pos_tags.to(device)  # Move to GPU\n",
        "    padded_pos_tags = [F.pad(torch.tensor(tags), pad=(0, max_length - len(tags))) for tags in encoded_pos_tags]\n",
        "    padded_pos_tags = torch.stack(padded_pos_tags).to(device)  # Stack the list of tensors into a single tensor and move to GPU\n",
        "    logging.debug(f\"padded_pos_tags: {padded_pos_tags}\")\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = [[float(i != 0.0) for i in seq] for seq in input_ids]\n",
        "    attention_masks = torch.tensor(attention_masks).to(device)  # Move to GPU\n",
        "    logging.debug(f\"attention_masks: {attention_masks}\")\n",
        "\n",
        "    print(f\"Number of sequences (dataset lines): {input_ids.shape[0]}\")\n",
        "    print()\n",
        "\n",
        "    return input_ids, padded_pos_tags, attention_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3cGhKC6Jjo9"
      },
      "source": [
        "### Mount Google drive and change active directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLuAkrkyJkKn",
        "outputId": "1364fe67-277a-480b-a356-0a247c3b7568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# change execution folder to MyDrive/UFMG/NLP/\n",
        "import os\n",
        "os.chdir('drive/MyDrive/UFMG/NLP/')\n",
        "# os.getcwd()\n",
        "# os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOEFJwqnzHS"
      },
      "source": [
        "### Carrega id, tags e masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74JpbpbslqYl"
      },
      "source": [
        "- O número de sentenças usadas no conjunto de treino foram de 728497 sentenças\n",
        "- Em um total de 37948 sequências, isto é, frases completas\n",
        "- O número de tags (classes) únicas é 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KUCLLN3JnaG",
        "outputId": "57a75c11-8cf0-4e2d-bc36-51b13108584d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Splitting data into sentences and POS tags: 100%|██████████| 37948/37948 [00:08<00:00, 4516.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 728497\n",
            "Number of unique tags: 26\n",
            "Number of sequences (dataset lines): 37948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Splitting data into sentences and POS tags: 100%|██████████| 1997/1997 [00:00<00:00, 5686.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 38881\n",
            "Number of unique tags: 26\n",
            "Number of sequences (dataset lines): 1997\n"
          ]
        }
      ],
      "source": [
        "with open('data/macmorpho-train.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    input_ids, tags, masks = preprocess_and_tokenize(data)\n",
        "\n",
        "with open('data/macmorpho-dev.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    val_input_ids, val_tags, val_masks = preprocess_and_tokenize(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZL9o0NGJqlt",
        "outputId": "e2128029-4333-4a6c-9e34-46bcbb81ec13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([37948, 512]), torch.Size([37948, 512]), 37948)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, tags.shape, len(masks)\n",
        "# len(val_input_ids), val_tags.shape, len(val_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcsWp3xcJla-"
      },
      "source": [
        "## Finetuning do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yJiqBFMJs9n"
      },
      "source": [
        "#### Modelo BERT `bert-base-multilingual-cased`\n",
        "- 179M parâmetros treináveis\n",
        "- Modelo pré-treinado em 104 idiomas, incluindo o português\n",
        "  - Versão \"cased\" do modelo, o que ajuda em POS Tagging\n",
        "- Modo de treinamento: Finetuning apenas na última camada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNtGe4oVJxnO",
        "outputId": "7f39973b-b0ed-4446-a9e2-9f2df179f02c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Define a custom PyTorch Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_ids, masks, tags):\n",
        "        self.input_ids = input_ids\n",
        "        self.masks = masks\n",
        "        self.tags = tags.to('cpu')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.masks[idx],\n",
        "            'labels': self.tags[idx]\n",
        "        }\n",
        "\n",
        "# Move the tensors to the CPU\n",
        "input_ids_cpu = input_ids.to('cpu')\n",
        "masks_cpu = masks.to('cpu')\n",
        "tags_cpu = tags.to('cpu')\n",
        "val_input_ids_cpu = val_input_ids.to('cpu')\n",
        "val_masks_cpu = val_masks.to('cpu')\n",
        "val_tags_cpu = val_tags.to('cpu')\n",
        "\n",
        "# Convert training data into PyTorch Dataset\n",
        "# train_dataset = CustomDataset(input_ids, masks, tags)\n",
        "train_dataset = CustomDataset(input_ids_cpu, masks_cpu, tags_cpu)\n",
        "\n",
        "# Convert validation data into PyTorch Dataset\n",
        "# eval_dataset = CustomDataset(val_input_ids, val_masks, val_tags)\n",
        "eval_dataset = CustomDataset(val_input_ids_cpu, val_masks_cpu, val_tags_cpu)\n",
        "\n",
        "# Initialize the BERT model\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    num_labels=max_length,\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16, # 4, 16\n",
        "    per_device_eval_batch_size=64,  # 4, 64\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1000,  # Set logging_steps to see the validation loss and metrics\n",
        "    # bf16=True, # or fp16, to reduce memory/computer usage/requirements\n",
        "    save_strategy=\"epoch\",\n",
        "    dataloader_num_workers=2,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Define the evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    labels = np.ravel(eval_pred.label_ids)\n",
        "    preds = np.ravel(eval_pred.predictions.argmax(-1))\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,  # Specify the evaluation metrics\n",
        ")\n",
        "\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogc-cFu5Jzk2",
        "outputId": "8f52c91e-e070-4765-c9db-9151a2464b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification_layer: Linear(in_features=768, out_features=512, bias=True)\n"
          ]
        }
      ],
      "source": [
        "classification_layer = model.classifier\n",
        "print(f\"classification_layer: {classification_layer}\")\n",
        "\n",
        "# Freeze all the layers of the BERT model\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Train only the classifier layer\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name, param.requires_grad)\n",
        "\n",
        "# Check that only the classifier layer is trainable\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' in name:  # Check if the parameter belongs to the classifier layer\n",
        "        assert param.requires_grad == True  # Assert that the classifier layer parameters are trainable\n",
        "    else:\n",
        "        assert param.requires_grad == False  # Assert that all other parameters are not trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "n6VrOEXRJ1sJ",
        "outputId": "36ee3e71-0da4-4671-e27f-e23810f7dcb8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4993' max='7116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4993/7116 51:23 < 21:51, 1.62 it/s, Epoch 2.10/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.214300</td>\n",
              "      <td>0.151001</td>\n",
              "      <td>0.964132</td>\n",
              "      <td>0.941663</td>\n",
              "      <td>0.964132</td>\n",
              "      <td>0.951356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.142800</td>\n",
              "      <td>0.130162</td>\n",
              "      <td>0.964526</td>\n",
              "      <td>0.946838</td>\n",
              "      <td>0.964526</td>\n",
              "      <td>0.954544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train_output = trainer.train()\n",
        "\n",
        "# Print the training loss\n",
        "print(train_output.training_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCgIPp8aeXra"
      },
      "outputs": [],
      "source": [
        "%tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU9-omxVJ38v"
      },
      "source": [
        "### Load test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyTpZHLZJ4cO"
      },
      "outputs": [],
      "source": [
        "with open('data/macmorpho-test.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    test_input_ids, test_tags, test_masks = preprocess_and_tokenize(data)\n",
        "\n",
        "test_input_ids_cpu = test_input_ids.to('cpu')\n",
        "test_masks_cpu = test_masks.to('cpu')\n",
        "test_tags_cpu = test_tags.to('cpu')\n",
        "\n",
        "# Convert test data into PyTorch Dataset\n",
        "test_dataset = CustomDataset(test_input_ids_cpu, test_masks_cpu, test_tags_cpu)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "results = trainer.evaluate(test_dataset)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpkqiDpzJ6eV"
      },
      "source": [
        "#### Salvar modelo treinado na downstream task (POS Tagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WvWw2F8J8CM"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned BERT model\n",
        "trainer.save_model(\"my_finetuned_bert_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvBtXlG_J9vV"
      },
      "source": [
        "# Referências\n",
        "- [Hugging Face: Transformers - BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
        "- [Hugging Face: BERT multilingual base model (cased)](https://huggingface.co/bert-base-multilingual-cased)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "D3cGhKC6Jjo9"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
