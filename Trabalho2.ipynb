{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre o trabalho\n",
    "- O trabalho tem os seguintes objetivos:\n",
    "    - Estudar a tarefa de **POS Tagging** (Part-Of-Speech (POS) Tagging) para a língua Portuguesa\n",
    "    - Classificar a classe gramatical de palavras em Português\n",
    "    - Implementar e avaliar a precisão de um modelo de POS tagging\n",
    "### Ferramentas, dados e bibliotecas utilizadas\n",
    "- O corpus de treino utilizado foi o corpus recomendado na descrição do trabalho\n",
    "- O corpus de avaliação utilizado foi o corpus recomendado na descrição do trabalho\n",
    "- A principal biblioteca utilizada para manipular os dados foi apenas a biblioteca recomendada: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabalho Prático II\n",
    "\n",
    "Nosso objetivo é estudar a tarefa de POS Tagging para a língua Portuguesa. Para isso utilizaremos o corpus Mac-Morpho, produzido pelo grupo NILC da ICMC USP. O Mac-Morpho oferece dados para treinamento, validação e teste de modelos preditivos, capazes de classificar a classe gramatical de palavras em Português. Para acessar o conjunto de classes, acesse http://nilc.icmc.usp.br/macmorpho/macmorpho-manual.pdf.\n",
    "\n",
    "\n",
    "O corpus está disponível em http://nilc.icmc.usp.br/macmorpho/macmorpho-v3.tgz. Você deverá implementar e avaliar a precisão de um modelo de POS tagging, a sua escolha. Voce pode utilizar pacotes que facilitem a implementação, como gensim, e transformers. Você deverá entregar documentação embutida em um notebook, detalhando a tarefa, a metodologia e os resultados. Sua análise deverá discutir quais as classes gramaticais para as quais a precisão é maior/menor. Não utilizaremos LLMs para essa tarefa, mas a sugestão é utilizar Transformers disponiveis e pre-treinados, em especial o BERT.\n",
    "\n",
    "Exemplo de dados de treino:\n",
    "```\n",
    "Jersei_N atinge_V média_N de_PREP Cr$_CUR 1,4_NUM milhão_N na_PREP+ART venda_N da_PREP+ART Pinhal_NPROP em_PREP São_NPROP Paulo_NPROP ._PU Programe_V sua_PROADJ viagem_N à_PREP+ART Exposição_NPROP Nacional_NPROP do_NPROP Zebu_NPROP ,_PU que_PRO-KS começa_V dia_N 25_N ._PU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Assume sentences is a list of sentences and pos_tags is a list of POS tag sequences\n",
    "# Each sentence and POS tag sequence should be a list of words/POS tags\n",
    "\n",
    "def preprocess_and_tokenize(data, max_length=512):\n",
    "    sentences = []\n",
    "    pos_tags = []\n",
    "\n",
    "    # Split data into sentences and POS tags\n",
    "    for line in tqdm(data, desc=\"Processing data\"):\n",
    "        words = []\n",
    "        tags = []\n",
    "        for word in line.split():\n",
    "            split_word = word.split('_')\n",
    "            words.append(split_word[0])\n",
    "            tags.append(split_word[1])\n",
    "        sentences.append(words)\n",
    "        pos_tags.append(tags)\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    tokenized_inputs = tokenizer(sentences, truncation=True, padding=True, is_split_into_words=True, max_length=max_length)\n",
    "\n",
    "    # Handle the POS tags\n",
    "    new_pos_tags = []\n",
    "    for sent_tags in pos_tags:\n",
    "        new_tags = []\n",
    "        for tag in sent_tags:\n",
    "            new_tags.append(tag)\n",
    "        new_pos_tags.append(new_tags)\n",
    "\n",
    "    # Encode the POS tags\n",
    "    encoded_pos_tags = [label_encoder.fit_transform(tags) for tags in new_pos_tags]\n",
    "\n",
    "    # Pad the POS tags\n",
    "    padded_pos_tags = pad_sequences(encoded_pos_tags, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = [[float(i != 0.0) for i in seq] for seq in tokenized_inputs[\"input_ids\"]]\n",
    "\n",
    "    return tokenized_inputs[\"input_ids\"], padded_pos_tags, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/macmorpho-train.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    input_ids, tags, masks = preprocess_and_tokenize(data)\n",
    "\n",
    "with open('data/macmorpho-dev.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    val_input_ids, val_tags, val_masks = preprocess_and_tokenize(data)\n",
    "\n",
    "with open('data/macmorpho-test.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    test_input_ids, test_tags, test_masks = preprocess_and_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n",
      "2023-12-09 17:59:16.154653: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 710, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 653, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 636, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 755, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file3omv_k83.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_fileofuah1x2.py\", line 19, in tf__call\n        loss = ag__.if_exp(ag__.ld(labels) is None, lambda: None, lambda: ag__.converted_call(ag__.ld(self).hf_compute_loss, (), dict(labels=ag__.ld(labels), logits=ag__.ld(logits)), fscope), 'labels is None')\n    File \"/tmp/__autograph_generated_fileofuah1x2.py\", line 19, in <lambda>\n        loss = ag__.if_exp(ag__.ld(labels) is None, lambda: None, lambda: ag__.converted_call(ag__.ld(self).hf_compute_loss, (), dict(labels=ag__.ld(labels), logits=ag__.ld(logits)), fscope), 'labels is None')\n    File \"/tmp/__autograph_generated_fileb5p5yn6o.py\", line 92, in tf__hf_compute_loss\n        ag__.if_stmt(ag__.ld(self).config.tf_legacy_loss, if_body_3, else_body_3, get_state_3, set_state_3, ('do_return', 'retval_', 'labels'), 2)\n    File \"/tmp/__autograph_generated_fileb5p5yn6o.py\", line 76, in else_body_3\n        unmasked_loss = ag__.converted_call(ag__.ld(loss_fn), (ag__.converted_call(ag__.ld(tf).nn.relu, (ag__.ld(labels),), None, fscope), ag__.ld(logits)), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_token_classification_5' (type TFBertForTokenClassification).\n    \n    in user code:\n    \n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1748, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1773, in call  *\n            loss = None if labels is None else self.hf_compute_loss(labels=labels, logits=logits)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 271, in hf_compute_loss  *\n            unmasked_loss = loss_fn(tf.nn.relu(labels), logits)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__  **\n            losses = call_fn(y_true, y_pred)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n            return ag_fn(y_true, y_pred, **self._fn_kwargs)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n            return backend.sparse_categorical_crossentropy(\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n            res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    \n        ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(16, 512) and logits.shape=(16, 267, 10)\n    \n    \n    Call arguments received by layer 'tf_bert_for_token_classification_5' (type TFBertForTokenClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(16, 267), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(16, 267), dtype=float32)'}\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=tf.Tensor(shape=(16, 512), dtype=int32)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 53\u001b[0m\n\u001b[1;32m     45\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TFTrainer(\n\u001b[1;32m     46\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     47\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     48\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     49\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py:569\u001b[0m, in \u001b[0;36mTFTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    566\u001b[0m     steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_training_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m=\u001b[39m iterations\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_logging \u001b[38;5;241m=\u001b[39m epoch_iter \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch\n",
      "File \u001b[0;32m~/miniconda3/envs/general/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filehzqk7jzq.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__distributed_training_steps\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m nb_instances_in_batch \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_compute_nb_instances, (ag__\u001b[38;5;241m.\u001b[39mld(batch),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_get_step_inputs, (ag__\u001b[38;5;241m.\u001b[39mld(batch), ag__\u001b[38;5;241m.\u001b[39mld(nb_instances_in_batch)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileym_4ut60.py:111\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients\u001b[0;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[1;32m    109\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreduced_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    110\u001b[0m reduced_labels \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreduced_labels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileym_4ut60.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients.<locals>.if_body_4\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_4\u001b[39m():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m features, labels\n\u001b[0;32m---> 18\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_instances_in_global_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileoxfzxyrh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__training_step\u001b[0;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 15\u001b[0m per_example_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m scaled_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(per_example_loss) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(nb_instances_in_global_batch),), \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(per_example_loss)\u001b[38;5;241m.\u001b[39mdtype), fscope)\n\u001b[1;32m     17\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mgradients, (ag__\u001b[38;5;241m.\u001b[39mld(scaled_loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileofdnpgpk.py:52\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model\u001b[0;34m(self, features, labels, training)\u001b[0m\n\u001b[1;32m     50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel, (ag__\u001b[38;5;241m.\u001b[39mld(features),), \u001b[38;5;28mdict\u001b[39m(labels\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(labels), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     51\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(outputs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileofdnpgpk.py:50\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model.<locals>.else_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_1\u001b[39m():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m outputs\n\u001b[0;32m---> 50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file3omv_k83.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileofuah1x2.py:19\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m     17\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (), \u001b[38;5;28mdict\u001b[39m(inputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(sequence_output), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     18\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mclassifier, (), \u001b[38;5;28mdict\u001b[39m(inputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(sequence_output)), fscope)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_compute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels is None\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileofuah1x2.py:19\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (), \u001b[38;5;28mdict\u001b[39m(inputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(sequence_output), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     18\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mclassifier, (), \u001b[38;5;28mdict\u001b[39m(inputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(sequence_output)), fscope)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp(ag__\u001b[38;5;241m.\u001b[39mld(labels) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_compute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels is None\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileb5p5yn6o.py:92\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__hf_compute_loss\u001b[0;34m(self, labels, logits)\u001b[0m\n\u001b[1;32m     90\u001b[0m unmasked_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munmasked_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m loss_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_mask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_legacy_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileb5p5yn6o.py:76\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__hf_compute_loss.<locals>.else_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_3\u001b[39m():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m retval_, labels, do_return\n\u001b[0;32m---> 76\u001b[0m     unmasked_loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     loss_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(labels) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,), \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unmasked_loss)\u001b[38;5;241m.\u001b[39mdtype), fscope)\n\u001b[1;32m     78\u001b[0m     masked_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(unmasked_loss) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss_mask)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 710, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 653, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 636, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/trainer_tf.py\", line 755, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file3omv_k83.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_fileofuah1x2.py\", line 19, in tf__call\n        loss = ag__.if_exp(ag__.ld(labels) is None, lambda: None, lambda: ag__.converted_call(ag__.ld(self).hf_compute_loss, (), dict(labels=ag__.ld(labels), logits=ag__.ld(logits)), fscope), 'labels is None')\n    File \"/tmp/__autograph_generated_fileofuah1x2.py\", line 19, in <lambda>\n        loss = ag__.if_exp(ag__.ld(labels) is None, lambda: None, lambda: ag__.converted_call(ag__.ld(self).hf_compute_loss, (), dict(labels=ag__.ld(labels), logits=ag__.ld(logits)), fscope), 'labels is None')\n    File \"/tmp/__autograph_generated_fileb5p5yn6o.py\", line 92, in tf__hf_compute_loss\n        ag__.if_stmt(ag__.ld(self).config.tf_legacy_loss, if_body_3, else_body_3, get_state_3, set_state_3, ('do_return', 'retval_', 'labels'), 2)\n    File \"/tmp/__autograph_generated_fileb5p5yn6o.py\", line 76, in else_body_3\n        unmasked_loss = ag__.converted_call(ag__.ld(loss_fn), (ag__.converted_call(ag__.ld(tf).nn.relu, (ag__.ld(labels),), None, fscope), ag__.ld(logits)), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_token_classification_5' (type TFBertForTokenClassification).\n    \n    in user code:\n    \n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1748, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1773, in call  *\n            loss = None if labels is None else self.hf_compute_loss(labels=labels, logits=logits)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 271, in hf_compute_loss  *\n            unmasked_loss = loss_fn(tf.nn.relu(labels), logits)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__  **\n            losses = call_fn(y_true, y_pred)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n            return ag_fn(y_true, y_pred, **self._fn_kwargs)\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n            return backend.sparse_categorical_crossentropy(\n        File \"/home/misaelrezende/miniconda3/envs/general/lib/python3.11/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n            res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    \n        ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(16, 512) and logits.shape=(16, 267, 10)\n    \n    \n    Call arguments received by layer 'tf_bert_for_token_classification_5' (type TFBertForTokenClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(16, 267), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(16, 267), dtype=float32)'}\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=tf.Tensor(shape=(16, 512), dtype=int32)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForTokenClassification, TFTrainingArguments, TFTrainer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set max_length\n",
    "max_length = 512\n",
    "\n",
    "# Convert training data into TF Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": input_ids, \"attention_mask\": masks},\n",
    "    tags\n",
    "))\n",
    "\n",
    "# Convert validation data into TF Dataset\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": val_input_ids, \"attention_mask\": val_masks},\n",
    "    val_tags\n",
    "))\n",
    "\n",
    "\n",
    "# Initialize the BERT model\n",
    "model = TFBertForTokenClassification.from_pretrained(\n",
    "            'bert-base-multilingual-cased',\n",
    "            num_labels=len(label_encoder.classes_),\n",
    "            max_length=max_length\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
