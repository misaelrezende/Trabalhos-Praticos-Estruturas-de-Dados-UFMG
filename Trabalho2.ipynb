{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzYRgmLXidaR"
      },
      "source": [
        "# Trabalho 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onAj28mVidaS"
      },
      "source": [
        "### Sobre o trabalho\n",
        "- O trabalho tem os seguintes objetivos:\n",
        "    - Estudar a tarefa de **POS Tagging** (Part-Of-Speech (POS) Tagging) para a língua Portuguesa\n",
        "    - Classificar a classe gramatical de palavras em Português\n",
        "    - Implementar e avaliar a precisão de um modelo de POS tagging\n",
        "\n",
        "### Resumo do trabalho\n",
        "- O modelo utilizado foi o BERT\n",
        "- Foi feito um finetuning no modelo, para classificação de classe gramatical\n",
        "\n",
        "### Ferramentas, dados e bibliotecas utilizadas\n",
        "- O corpus de treino, avaliação e teste utilizado foi o corpus recomendado na descrição do trabalho: macmorpho\n",
        "- A principal biblioteca utilizada para manipular os dados foi a biblioteca recomendada: `transformers`\n",
        "  - Outras bibliotecas úteis utilizadas: `torch`, `numpy` e `scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwXfNu8sidaT"
      },
      "source": [
        "Trabalho Prático II\n",
        "\n",
        "Nosso objetivo é estudar a tarefa de POS Tagging para a língua Portuguesa. Para isso utilizaremos o corpus Mac-Morpho, produzido pelo grupo NILC da ICMC USP. O Mac-Morpho oferece dados para treinamento, validação e teste de modelos preditivos, capazes de classificar a classe gramatical de palavras em Português. Para acessar o conjunto de classes, acesse http://nilc.icmc.usp.br/macmorpho/macmorpho-manual.pdf.\n",
        "\n",
        "\n",
        "O corpus está disponível em http://nilc.icmc.usp.br/macmorpho/macmorpho-v3.tgz. Você deverá implementar e avaliar a precisão de um modelo de POS tagging, a sua escolha. Voce pode utilizar pacotes que facilitem a implementação, como gensim, e transformers. Você deverá entregar documentação embutida em um notebook, detalhando a tarefa, a metodologia e os resultados. Sua análise deverá discutir quais as classes gramaticais para as quais a precisão é maior/menor. Não utilizaremos LLMs para essa tarefa, mas a sugestão é utilizar Transformers disponiveis e pre-treinados, em especial o BERT.\n",
        "\n",
        "Exemplo de dados de treino:\n",
        "```\n",
        "Jersei_N atinge_V média_N de_PREP Cr$_CUR 1,4_NUM milhão_N na_PREP+ART venda_N da_PREP+ART Pinhal_NPROP em_PREP São_NPROP Paulo_NPROP ._PU Programe_V sua_PROADJ viagem_N à_PREP+ART Exposição_NPROP Nacional_NPROP do_NPROP Zebu_NPROP ,_PU que_PRO-KS começa_V dia_N 25_N ._PU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussão\n",
        "\n",
        "#### Geral\n",
        "- Quantas sentenças no treino, validação e teste?\n",
        "- Quantas tags diferentes? Quantos parametros treináveis tem o modelo no total?\n",
        "\n",
        "#### Específico para avaliação do trabalho\n",
        "- `Implementar e avaliar a precisão:` isso pra mim é falar de acurácia.\n",
        "- `detalhando a tarefa, metodologia e os resultados`\n",
        "- `discutir quais classes gramaticais tem maior/menor precisão`"
      ],
      "metadata": {
        "id": "sKxUNLkDNnu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install transformers[torch]\n",
        "%pip install accelerate -U --quiet"
      ],
      "metadata": {
        "id": "AoTsbp4pnAVb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregar e tokenizar os dados de treino"
      ],
      "metadata": {
        "id": "ZaChStAaRaqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L0r6a6AuidaW"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Create a device object\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Set max_length\n",
        "max_length = 512\n",
        "\n",
        "def preprocess_and_tokenize(data):\n",
        "    sentences = []\n",
        "    pos_tags = []\n",
        "\n",
        "    # Split data into sentences and POS tags\n",
        "    for line in tqdm(data, desc=\"Splitting data into sentences and POS tags\"):\n",
        "        words = []\n",
        "        tags = []\n",
        "        for word in line.split():\n",
        "            split_word = word.split('_')\n",
        "            words.append(split_word[0])\n",
        "            tags.append(split_word[1])\n",
        "        sentences.append(words)\n",
        "        pos_tags.append(tags)\n",
        "\n",
        "    # Tokenize the sentences\n",
        "    tokenized_inputs = bert_tokenizer(sentences, truncation=True, padding='max_length',\n",
        "                                      max_length=max_length, is_split_into_words=True)\n",
        "\n",
        "    # Pad input_ids to max_length and convert to a numpy array\n",
        "    input_ids = pad_sequence([torch.tensor(i) for i in tokenized_inputs[\"input_ids\"]], batch_first=True)\n",
        "    # input_ids = input_ids.numpy()\n",
        "    input_ids = input_ids.to(device)  # Move to GPU\n",
        "    print(f\"Shape of input_ids: {input_ids.shape}\")\n",
        "\n",
        "    # Handle the POS tags\n",
        "    new_pos_tags = []\n",
        "    for sent_tags, input_id in zip(pos_tags, tokenized_inputs[\"input_ids\"]):\n",
        "        new_tags = []\n",
        "        for tag in sent_tags:\n",
        "            new_tags.extend([tag] * len(input_id))\n",
        "        new_pos_tags.append(new_tags[:len(input_id)])\n",
        "\n",
        "    # Encode the POS tags\n",
        "    encoded_pos_tags = [label_encoder.fit_transform(tags) for tags in new_pos_tags]\n",
        "\n",
        "    # Pad the POS tags\n",
        "    padded_pos_tags = pad_sequence([torch.tensor(tags) for tags in encoded_pos_tags], batch_first=True)\n",
        "    # padded_pos_tags = padded_pos_tags.numpy()\n",
        "    padded_pos_tags = padded_pos_tags.to(device)  # Move to GPU\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = [[float(i != 0.0) for i in seq] for seq in input_ids]\n",
        "    attention_masks = torch.tensor(attention_masks).to(device)  # Move to GPU\n",
        "    # print(f\"Shape of tokenized inputs: {tokenized_inputs['input_ids'].shape}\")\n",
        "    # return tokenized_inputs[\"input_ids\"], padded_pos_tags, attention_masks\n",
        "    return input_ids, padded_pos_tags, attention_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google drive and change active directory"
      ],
      "metadata": {
        "id": "-XO8NHJ1k0_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# change execution folder to MyDrive/UFMG/NLP/\n",
        "import os\n",
        "os.chdir('drive/MyDrive/UFMG/NLP/')\n",
        "# os.getcwd()\n",
        "# os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dm2Qwg7jpId",
        "outputId": "0f2ef3a0-8984-47ee-8e31-6df298649e96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/macmorpho-train.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    input_ids, tags, masks = preprocess_and_tokenize(data)\n",
        "\n",
        "with open('data/macmorpho-dev.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    val_input_ids, val_tags, val_masks = preprocess_and_tokenize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-ENXMqvjkwt",
        "outputId": "7be9c80f-f846-4b73-a052-73c176fa7029"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting data into sentences and POS tags: 100%|██████████| 37948/37948 [00:00<00:00, 81170.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input_ids: torch.Size([37948, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting data into sentences and POS tags: 100%|██████████| 1997/1997 [00:00<00:00, 95580.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input_ids: torch.Size([1997, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-WF0WnFidaY",
        "outputId": "04aeb798-ce0b-4a56-c487-b9a58da28709"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([37948, 512]), torch.Size([37948, 512]), 37948)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "input_ids.shape, tags.shape, len(masks)\n",
        "# len(val_input_ids), val_tags.shape, len(val_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning do modelo"
      ],
      "metadata": {
        "id": "boGbLjbCSS2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modelo BERT `bert-base-multilingual-cased`\n",
        "- 179M parâmetros treináveis\n",
        "- Modelo pré-treinado em 104 idiomas, incluindo o português\n",
        "- Modo de treinamento: Finetuning apenas na última camada"
      ],
      "metadata": {
        "id": "soJEHn-OSYEU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTgHYCaVidaZ",
        "outputId": "6e9ed284-c637-4dd9-a56f-152ed43006d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=512, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Define a custom PyTorch Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_ids, masks, tags):\n",
        "        self.input_ids = input_ids\n",
        "        self.masks = masks\n",
        "        self.tags = tags.to('cpu')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.masks[idx],\n",
        "            'labels': self.tags[idx]\n",
        "        }\n",
        "\n",
        "# Move the tensors to the CPU\n",
        "input_ids_cpu = input_ids.to('cpu')\n",
        "masks_cpu = masks.to('cpu')\n",
        "tags_cpu = tags.to('cpu')\n",
        "val_input_ids_cpu = val_input_ids.to('cpu')\n",
        "val_masks_cpu = val_masks.to('cpu')\n",
        "val_tags_cpu = val_tags.to('cpu')\n",
        "\n",
        "# Convert training data into PyTorch Dataset\n",
        "# train_dataset = CustomDataset(input_ids, masks, tags)\n",
        "train_dataset = CustomDataset(input_ids_cpu, masks_cpu, tags_cpu)\n",
        "\n",
        "# Convert validation data into PyTorch Dataset\n",
        "# eval_dataset = CustomDataset(val_input_ids, val_masks, val_tags)\n",
        "eval_dataset = CustomDataset(val_input_ids_cpu, val_masks_cpu, val_tags_cpu)\n",
        "\n",
        "# Initialize the BERT model\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    num_labels=512,\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4, # 16\n",
        "    per_device_eval_batch_size=4,  # 64\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    # device=device,\n",
        "    # accelerator='cpu'\n",
        "    # bf16=True, # or fp16, to reduce memory/computer usage/requirements\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_layer = model.classifier\n",
        "print(f\"classification_layer: {classification_layer}\")\n",
        "\n",
        "# Freeze all the layers of the BERT model\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Train only the classifier layer\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name, param.requires_grad)\n",
        "\n",
        "# Check that only the classifier layer is trainable\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' in name:  # Check if the parameter belongs to the classifier layer\n",
        "        assert param.requires_grad == True  # Assert that the classifier layer parameters are trainable\n",
        "    else:\n",
        "        assert param.requires_grad == False  # Assert that all other parameters are not trainable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiqU8Zl9G9ND",
        "outputId": "0863c32a-bf75-4e30-c00c-30b73edf851b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=768, out_features=512, bias=True)\n",
            "bert.embeddings.word_embeddings.weight False\n",
            "bert.embeddings.position_embeddings.weight False\n",
            "bert.embeddings.token_type_embeddings.weight False\n",
            "bert.embeddings.LayerNorm.weight False\n",
            "bert.embeddings.LayerNorm.bias False\n",
            "bert.encoder.layer.0.attention.self.query.weight False\n",
            "bert.encoder.layer.0.attention.self.query.bias False\n",
            "bert.encoder.layer.0.attention.self.key.weight False\n",
            "bert.encoder.layer.0.attention.self.key.bias False\n",
            "bert.encoder.layer.0.attention.self.value.weight False\n",
            "bert.encoder.layer.0.attention.self.value.bias False\n",
            "bert.encoder.layer.0.attention.output.dense.weight False\n",
            "bert.encoder.layer.0.attention.output.dense.bias False\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.0.intermediate.dense.weight False\n",
            "bert.encoder.layer.0.intermediate.dense.bias False\n",
            "bert.encoder.layer.0.output.dense.weight False\n",
            "bert.encoder.layer.0.output.dense.bias False\n",
            "bert.encoder.layer.0.output.LayerNorm.weight False\n",
            "bert.encoder.layer.0.output.LayerNorm.bias False\n",
            "bert.encoder.layer.1.attention.self.query.weight False\n",
            "bert.encoder.layer.1.attention.self.query.bias False\n",
            "bert.encoder.layer.1.attention.self.key.weight False\n",
            "bert.encoder.layer.1.attention.self.key.bias False\n",
            "bert.encoder.layer.1.attention.self.value.weight False\n",
            "bert.encoder.layer.1.attention.self.value.bias False\n",
            "bert.encoder.layer.1.attention.output.dense.weight False\n",
            "bert.encoder.layer.1.attention.output.dense.bias False\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.1.intermediate.dense.weight False\n",
            "bert.encoder.layer.1.intermediate.dense.bias False\n",
            "bert.encoder.layer.1.output.dense.weight False\n",
            "bert.encoder.layer.1.output.dense.bias False\n",
            "bert.encoder.layer.1.output.LayerNorm.weight False\n",
            "bert.encoder.layer.1.output.LayerNorm.bias False\n",
            "bert.encoder.layer.2.attention.self.query.weight False\n",
            "bert.encoder.layer.2.attention.self.query.bias False\n",
            "bert.encoder.layer.2.attention.self.key.weight False\n",
            "bert.encoder.layer.2.attention.self.key.bias False\n",
            "bert.encoder.layer.2.attention.self.value.weight False\n",
            "bert.encoder.layer.2.attention.self.value.bias False\n",
            "bert.encoder.layer.2.attention.output.dense.weight False\n",
            "bert.encoder.layer.2.attention.output.dense.bias False\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.2.intermediate.dense.weight False\n",
            "bert.encoder.layer.2.intermediate.dense.bias False\n",
            "bert.encoder.layer.2.output.dense.weight False\n",
            "bert.encoder.layer.2.output.dense.bias False\n",
            "bert.encoder.layer.2.output.LayerNorm.weight False\n",
            "bert.encoder.layer.2.output.LayerNorm.bias False\n",
            "bert.encoder.layer.3.attention.self.query.weight False\n",
            "bert.encoder.layer.3.attention.self.query.bias False\n",
            "bert.encoder.layer.3.attention.self.key.weight False\n",
            "bert.encoder.layer.3.attention.self.key.bias False\n",
            "bert.encoder.layer.3.attention.self.value.weight False\n",
            "bert.encoder.layer.3.attention.self.value.bias False\n",
            "bert.encoder.layer.3.attention.output.dense.weight False\n",
            "bert.encoder.layer.3.attention.output.dense.bias False\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.3.intermediate.dense.weight False\n",
            "bert.encoder.layer.3.intermediate.dense.bias False\n",
            "bert.encoder.layer.3.output.dense.weight False\n",
            "bert.encoder.layer.3.output.dense.bias False\n",
            "bert.encoder.layer.3.output.LayerNorm.weight False\n",
            "bert.encoder.layer.3.output.LayerNorm.bias False\n",
            "bert.encoder.layer.4.attention.self.query.weight False\n",
            "bert.encoder.layer.4.attention.self.query.bias False\n",
            "bert.encoder.layer.4.attention.self.key.weight False\n",
            "bert.encoder.layer.4.attention.self.key.bias False\n",
            "bert.encoder.layer.4.attention.self.value.weight False\n",
            "bert.encoder.layer.4.attention.self.value.bias False\n",
            "bert.encoder.layer.4.attention.output.dense.weight False\n",
            "bert.encoder.layer.4.attention.output.dense.bias False\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.4.intermediate.dense.weight False\n",
            "bert.encoder.layer.4.intermediate.dense.bias False\n",
            "bert.encoder.layer.4.output.dense.weight False\n",
            "bert.encoder.layer.4.output.dense.bias False\n",
            "bert.encoder.layer.4.output.LayerNorm.weight False\n",
            "bert.encoder.layer.4.output.LayerNorm.bias False\n",
            "bert.encoder.layer.5.attention.self.query.weight False\n",
            "bert.encoder.layer.5.attention.self.query.bias False\n",
            "bert.encoder.layer.5.attention.self.key.weight False\n",
            "bert.encoder.layer.5.attention.self.key.bias False\n",
            "bert.encoder.layer.5.attention.self.value.weight False\n",
            "bert.encoder.layer.5.attention.self.value.bias False\n",
            "bert.encoder.layer.5.attention.output.dense.weight False\n",
            "bert.encoder.layer.5.attention.output.dense.bias False\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.5.intermediate.dense.weight False\n",
            "bert.encoder.layer.5.intermediate.dense.bias False\n",
            "bert.encoder.layer.5.output.dense.weight False\n",
            "bert.encoder.layer.5.output.dense.bias False\n",
            "bert.encoder.layer.5.output.LayerNorm.weight False\n",
            "bert.encoder.layer.5.output.LayerNorm.bias False\n",
            "bert.encoder.layer.6.attention.self.query.weight False\n",
            "bert.encoder.layer.6.attention.self.query.bias False\n",
            "bert.encoder.layer.6.attention.self.key.weight False\n",
            "bert.encoder.layer.6.attention.self.key.bias False\n",
            "bert.encoder.layer.6.attention.self.value.weight False\n",
            "bert.encoder.layer.6.attention.self.value.bias False\n",
            "bert.encoder.layer.6.attention.output.dense.weight False\n",
            "bert.encoder.layer.6.attention.output.dense.bias False\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.6.intermediate.dense.weight False\n",
            "bert.encoder.layer.6.intermediate.dense.bias False\n",
            "bert.encoder.layer.6.output.dense.weight False\n",
            "bert.encoder.layer.6.output.dense.bias False\n",
            "bert.encoder.layer.6.output.LayerNorm.weight False\n",
            "bert.encoder.layer.6.output.LayerNorm.bias False\n",
            "bert.encoder.layer.7.attention.self.query.weight False\n",
            "bert.encoder.layer.7.attention.self.query.bias False\n",
            "bert.encoder.layer.7.attention.self.key.weight False\n",
            "bert.encoder.layer.7.attention.self.key.bias False\n",
            "bert.encoder.layer.7.attention.self.value.weight False\n",
            "bert.encoder.layer.7.attention.self.value.bias False\n",
            "bert.encoder.layer.7.attention.output.dense.weight False\n",
            "bert.encoder.layer.7.attention.output.dense.bias False\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.7.intermediate.dense.weight False\n",
            "bert.encoder.layer.7.intermediate.dense.bias False\n",
            "bert.encoder.layer.7.output.dense.weight False\n",
            "bert.encoder.layer.7.output.dense.bias False\n",
            "bert.encoder.layer.7.output.LayerNorm.weight False\n",
            "bert.encoder.layer.7.output.LayerNorm.bias False\n",
            "bert.encoder.layer.8.attention.self.query.weight False\n",
            "bert.encoder.layer.8.attention.self.query.bias False\n",
            "bert.encoder.layer.8.attention.self.key.weight False\n",
            "bert.encoder.layer.8.attention.self.key.bias False\n",
            "bert.encoder.layer.8.attention.self.value.weight False\n",
            "bert.encoder.layer.8.attention.self.value.bias False\n",
            "bert.encoder.layer.8.attention.output.dense.weight False\n",
            "bert.encoder.layer.8.attention.output.dense.bias False\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.8.intermediate.dense.weight False\n",
            "bert.encoder.layer.8.intermediate.dense.bias False\n",
            "bert.encoder.layer.8.output.dense.weight False\n",
            "bert.encoder.layer.8.output.dense.bias False\n",
            "bert.encoder.layer.8.output.LayerNorm.weight False\n",
            "bert.encoder.layer.8.output.LayerNorm.bias False\n",
            "bert.encoder.layer.9.attention.self.query.weight False\n",
            "bert.encoder.layer.9.attention.self.query.bias False\n",
            "bert.encoder.layer.9.attention.self.key.weight False\n",
            "bert.encoder.layer.9.attention.self.key.bias False\n",
            "bert.encoder.layer.9.attention.self.value.weight False\n",
            "bert.encoder.layer.9.attention.self.value.bias False\n",
            "bert.encoder.layer.9.attention.output.dense.weight False\n",
            "bert.encoder.layer.9.attention.output.dense.bias False\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.9.intermediate.dense.weight False\n",
            "bert.encoder.layer.9.intermediate.dense.bias False\n",
            "bert.encoder.layer.9.output.dense.weight False\n",
            "bert.encoder.layer.9.output.dense.bias False\n",
            "bert.encoder.layer.9.output.LayerNorm.weight False\n",
            "bert.encoder.layer.9.output.LayerNorm.bias False\n",
            "bert.encoder.layer.10.attention.self.query.weight False\n",
            "bert.encoder.layer.10.attention.self.query.bias False\n",
            "bert.encoder.layer.10.attention.self.key.weight False\n",
            "bert.encoder.layer.10.attention.self.key.bias False\n",
            "bert.encoder.layer.10.attention.self.value.weight False\n",
            "bert.encoder.layer.10.attention.self.value.bias False\n",
            "bert.encoder.layer.10.attention.output.dense.weight False\n",
            "bert.encoder.layer.10.attention.output.dense.bias False\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.10.intermediate.dense.weight False\n",
            "bert.encoder.layer.10.intermediate.dense.bias False\n",
            "bert.encoder.layer.10.output.dense.weight False\n",
            "bert.encoder.layer.10.output.dense.bias False\n",
            "bert.encoder.layer.10.output.LayerNorm.weight False\n",
            "bert.encoder.layer.10.output.LayerNorm.bias False\n",
            "bert.encoder.layer.11.attention.self.query.weight False\n",
            "bert.encoder.layer.11.attention.self.query.bias False\n",
            "bert.encoder.layer.11.attention.self.key.weight False\n",
            "bert.encoder.layer.11.attention.self.key.bias False\n",
            "bert.encoder.layer.11.attention.self.value.weight False\n",
            "bert.encoder.layer.11.attention.self.value.bias False\n",
            "bert.encoder.layer.11.attention.output.dense.weight False\n",
            "bert.encoder.layer.11.attention.output.dense.bias False\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.11.intermediate.dense.weight False\n",
            "bert.encoder.layer.11.intermediate.dense.bias False\n",
            "bert.encoder.layer.11.output.dense.weight False\n",
            "bert.encoder.layer.11.output.dense.bias False\n",
            "bert.encoder.layer.11.output.LayerNorm.weight False\n",
            "bert.encoder.layer.11.output.LayerNorm.bias False\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-XKr5XNnGsYo",
        "outputId": "ad0fe544-542d-4739-ad08-0526be8ed3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22001' max='28461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22001/28461 57:40 < 16:56, 6.36 it/s, Epoch 2.32/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.195500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.468400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.101100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.009800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.004300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip list transfomers"
      ],
      "metadata": {
        "id": "JWLlsql4mRW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk_y3hpwidaa"
      },
      "source": [
        "### Load test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwrmiv_Tidaa"
      },
      "outputs": [],
      "source": [
        "with open('data/macmorpho-test.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    test_input_ids, test_tags, test_masks = preprocess_and_tokenize(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGM5sscjidac"
      },
      "source": [
        "# Referências\n",
        "- [Hugging Face: Transformers - BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
        "- [Hugging Face: BERT multilingual base model (cased)](https://huggingface.co/bert-base-multilingual-cased)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}